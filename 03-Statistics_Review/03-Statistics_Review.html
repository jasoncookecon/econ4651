<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistics Review II</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Cook" />
    <link href="03-Statistics_Review_files/remark-css/default.css" rel="stylesheet" />
    <link href="03-Statistics_Review_files/remark-css/metropolis.css" rel="stylesheet" />
    <link href="03-Statistics_Review_files/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Statistics Review II
## ECON 4651: Principles of Econometrics for Business and Analytics
### Jason Cook
### Fall 2020

---

class: inverse, middle



# Prologue

&lt;!-- install.packages(c("pagedown", "xaringan")) --&gt;
&lt;!-- pagedown::chrome_print("C:/Users/Jason/Box/Teaching/Jason Cook/QAMO_UG_Intro_Metrics/Lectures/02-Statistics_Review/02-Statistics_Review.html") --&gt;
---
# Housekeeping

Problem Set 1 available on Canvas. Due 9/3 by 5pm.

- Message Blake if you want a group and need help finding one

---
class: inverse, middle

# Statistics Review

---
# Overview

__Goal:__ Learn about a population.

- In particular, learn about an unknown population .hi[parameter].

__Challenge:__ Usually cannot access information about the entire population.

__Solution:__ Sample from the population and estimate the parameter.

- Draw `\(n\)` observations from the population, then use an estimator.

---
# Sampling

There are myriad ways to produce a sample,&lt;sup&gt;*&lt;/sup&gt; but we will restrict our attention to __simple random sampling__, where

1. Each observation is a random variable.

2. The `\(n\)` random variables are independent.

3. Life becomes much simpler for the econometrician.

.footnote[
&lt;sup&gt;*&lt;/sup&gt; Only a subset of these can help produce reliable statistics.
]

---
# Estimators

An __estimator__ is a rule (or formula) for estimating an unknown population parameter given a sample of data.

--

- Each observation in the sample is a random variable.

--

- An estimator is a combination of random variables `\(\implies\)` it is a random variable.

__Example:__ Sample mean

$$
\bar{X} = \dfrac{1}{n} \sum_{i=1}^n X_i
$$

- `\(\bar{X}\)` is an estimator for the population mean `\(\mu\)`.

- Given a sample, `\(\bar{X}\)` yields an __estimate__ `\(\bar{x}\)` or `\(\hat{\mu}\)`, a specific number.

---
class: clear-slide, middle

You can think of estimators as trying to hit a bulls-eye at an archery range...



&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

---
class: clear-slide, middle
count: false

You can think of estimators as trying to hit a bulls-eye at an archery range...

&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

---
class: clear-slide, middle
count: false

You can think of estimators as trying to hit a bulls-eye at an archery range...

&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;

---
class: clear-slide, middle

.pull-left[
.center[**Archer 1**]
&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-5-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Archer 3**]
&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-6-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
.center[**Archer 2**]
&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-7-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Archer 4**]
&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?



.pull-left[

&lt;img src="03-Statistics_Review_files/figure-html/pop1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Population**]

]

--

.pull-right[

&lt;img src="03-Statistics_Review_files/figure-html/mean1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Population relationship**]
&lt;br&gt;
`\(\mu = 3.75\)`

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

&lt;img src="03-Statistics_Review_files/figure-html/sample1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 1:** 10 random individuals]

]

--

.pull-right[

&lt;img src="03-Statistics_Review_files/figure-html/sample1 mean-1.svg" style="display: block; margin: auto;" /&gt;

.center[

**Population relationship**
&lt;br&gt;
`\(\mu = 3.75\)`

**Sample relationship**
&lt;br&gt;
`\(\hat{\mu} = 5.19\)`

]

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

&lt;img src="03-Statistics_Review_files/figure-html/sample2-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 2:** 10 random individuals]

]

--

.pull-right[

&lt;img src="03-Statistics_Review_files/figure-html/sample2 mean-1.svg" style="display: block; margin: auto;" /&gt;

.center[

**Population relationship**
&lt;br&gt;
`\(\mu = 3.75\)`

**Sample relationship**
&lt;br&gt;
`\(\hat{\mu} = 2.79\)`

]

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

&lt;img src="03-Statistics_Review_files/figure-html/sample3-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 3:** 10 random individuals]

]

--

.pull-right[

&lt;img src="03-Statistics_Review_files/figure-html/sample3 mean-1.svg" style="display: block; margin: auto;" /&gt;

.center[

**Population relationship**
&lt;br&gt;
`\(\mu = 3.75\)`

**Sample relationship**
&lt;br&gt;
`\(\hat{\mu} = 0.67\)`

]

]

---
class: clear-slide, middle

Let's repeat this **10,000 times** and then plot the estimates.

(This exercise is called a Monte Carlo simulation.)

---
class: clear-slide, middle

&lt;img src="03-Statistics_Review_files/figure-html/simulation-1.svg" style="display: block; margin: auto;" /&gt;

.center[__Sampling Distribution__]
.center[(more on this momentarily)]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[
&lt;img src="03-Statistics_Review_files/figure-html/simulation2-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[

- Mean of the samples are close to the population mean.

- But...some individual samples can miss the mark.

- The difference between individual samples and the population creates __uncertainty__. 

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

**Answer:** Uncertainty matters.

- `\(\hat{\mu}\)` is a random variable that depends on the sample.

- In practice, we don't know whether our sample is similar to the population or not. 

- Individual samples may have means that differ greatly from the population.

- We will have to keep track of this uncertainty.

- To do so, we need to discuss __Sampling Distributions__

--

- But first... 

---
# Group Questions

__Describe in your own words what the following terms are and how they connect to each other:__
  - **Population**
  - **Sample**
  - **Parameter**
  - **Estimator**

&lt;!-- --- --&gt;
&lt;!-- # Samples `\(\rightarrow\)` Populations --&gt;

&lt;!-- - It turns out sample estimates provide a great deal of info about populations --&gt;

&lt;!-- - Even more amazingly, we can quantify how certain we should be about our estimates --&gt;

&lt;!-- - This is made possible by two amazing statistical theorems, the _Law of Large Numbers_ and the _Central Limit Theorem_ --&gt;

&lt;!-- --- --&gt;
&lt;!-- # Samples `\(\rightarrow\)` Populations --&gt;

&lt;!-- - It was not an accident that the sample mean was a good estimator for the population mean --&gt;

&lt;!-- - Let's see why --&gt;

&lt;!-- -- --&gt;

&lt;!-- - Consider a single sample average, e.g., average height of a sample of ![:scale 4%](figs/BlockU.png) students --&gt;
&lt;!--   + Recall, __Sample Mean__: `\(\hat{\mu}=Avg_n(Y_i) = \frac{1}{n}\sum_{i=1}^n Y_i\)` --&gt;
&lt;!-- -- --&gt;

&lt;!-- - Want to know the corresponding __population mean__, e.g., average height of all ![:scale 4%](figs/BlockU.png) students, aka, __Expectation__ or __Expected Value__, `\(\mu\)`, or `\(E[Y_i]\)` --&gt;

&lt;!-- - Recall, can write Expectations as weighted averages of all possible `\(Y_i\)` values --&gt;
&lt;!--   + Weights given by probability that values appear in the population  --&gt;

&lt;!-- --- --&gt;

&lt;!-- # Samples `\(\rightarrow\)` Populations --&gt;
&lt;!-- ## Definition - Expected Value --&gt;
&lt;!-- &gt; Suppose random variable `\(Y\)` has `\(k\)` possible values, where `\(y_i\)` is the `\(i_{\text{th}}\)` value and `\(p_i\)` is the probability that `\(Y\)` takes on `\(y_i\)`, then `$$E[Y]=y_1p_1 + y_2p_2 + \cdots + y_kp_k = \sum_{i=1}^k y_ip_i$$` --&gt;

&lt;!-- -- --&gt;

&lt;!-- - __Example:__ Suppose I flip a coin and pay you $100 if heads or $0 if tails, then  --&gt;

&lt;!--   + `\(E[Y]=\frac{1}{2}*100 + \frac{1}{2}*0=50\)` --&gt;
&lt;!--   + Now suppose we play the game 10 times, and win 4 heads, then `\(Avg_{10}(Y_i)=\frac{1}{10}(100*4 + 0*6)=40\)` --&gt;

&lt;!-- - .pink[__Practice Problems (3)__] --&gt;

&lt;!-- --- --&gt;

&lt;!-- # Samples `\(\rightarrow\)` Populations --&gt;
&lt;!-- - `\(E[Y_i]\)` is fixed for a particular population and is called a __parameter__ --&gt;

&lt;!-- - The sample average of `\(Y_i\)`, `\(Avg_n(Y_i)\)`, aka `\(\bar{Y}\)`, varies from one sample to another --&gt;

&lt;!-- - `\(\bar{Y}\)` is a good estimator of `\(E[Y_i]\)`, __why?__ --&gt;

&lt;!-- -- --&gt;

---
class: inverse, middle

# Sampling Distribution

---
# Sampling Distribution
## Recap
- We have a _sample_ mean and we are trying to learn about a _population_ mean, but we know there will be uncertainty

## E.g., Average School Size in CA
- Suppose you want to know the average school size in California

- You can imagine that if we took a different samples of schools, average size would be different

- If you did this many times this would create a distribution, we call this the __sampling distribution__

---
# Sampling Distribution
## Number of Students in CA Schools
- To illustrate, consider data on the \# of students in California schools
- Here is the distribution -- heavily skewed
&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;" /&gt;
---
# Sampling Distribution
## Average \# of Students in CA Schools
- Now suppose we take different samples of schools, calculate the average, and kept track
--
&lt;img src="figs/CLT_CAschool.gif" width="55%" style="display: block; margin: auto;" /&gt;
---
#Sampling Distribution
.pull-left[
&lt;img src="figs/CLT_CAschool.gif" width="100%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
- This is the __sampling distribution__ of our estimator (_the sample average_) for the parameter (_the population average_) 
  + i.e., distribution of all possible sample averages
]

---
#Sampling Distribution
.pull-left[
&lt;img src="figs/CLT_CAschool.gif" width="100%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
## __Note:__
- Data are skewed, but sampling distribution is __normally distributed__ (bell curve)
- Mean of distribution is close to the  population average (~2,500)
- Spread of sampling distribution conveys _uncertainty_
  + i.e., more spread means higher chance any given sample is far away from truth
]

--
.hi[Group Question:] **What is the difference between the distribution of `\(Y\)` and the sampling distribution of `\(\bar{Y}\)`?**

---
class: inverse, middle

# Properties of Estimators

---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 1: Unbiasedness.**

.pull-left[

**Unbiased estimator:** `\(\mathop{\mathbb{E}}\left[ \hat{\mu} \right] = \mu\)`

&lt;img src="03-Statistics_Review_files/figure-html/unbiased pdf-1.svg" style="display: block; margin: auto;" /&gt;

]

--

.pull-right[

**Biased estimator:** `\(\mathop{\mathbb{E}}\left[ \hat{\mu} \right] \neq \mu\)`

&lt;img src="03-Statistics_Review_files/figure-html/biased pdf-1.svg" style="display: block; margin: auto;" /&gt;

]

--

- I.e., expected value of sampling distribution = true population parameter

---
# Properties of Estimators
## Sample Average is Unbiased

- Sample average turns out to be unbiased estimator of population average

- Recall: `\(\hat{\mu} \equiv \bar{Y} \equiv \frac{1}{n}\sum_{i=1}^{n} Y_i\ \text{ and }\ \mu \equiv \mathop{\mathbb{E}}[Y]\)`

--

- __Proof__: WTS `\(\mathop{\mathbb{E}}\left[ \hat{\mu} \right] = \mu\)` `$$\mathop{\mathbb{E}}[\hat{\mu}]=\mathop{\mathbb{E}}\left[\frac{1}{n}\sum_{i=1}^{n} Y_i\right]=\frac{1}{n}\sum_{i=1}^{n} \mathop{\mathbb{E}}\left[Y_i\right]=\frac{1}{n}\sum_{i=1}^{n} \mu= \frac{1}{n} n \mu=\mu.\blacksquare$$`

  + By simple properties of expectations (first lecture)

---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 2: Low Sampling Variance (a.k.a. Efficiency).**

The central tendencies (means) of competing distributions are not the only things that matter. We also care about the **variance** of an estimator, aka, __sampling variance__ (variance of _sampling distribution_).

$$ \mathop{\text{Var}} \left( \hat{\mu} \right) = \mathop{\mathbb{E}}\left[ \left( \hat{\mu} - \mathop{\mathbb{E}}\left[ \hat{\mu} \right] \right)^2 \right] $$

Lower variance estimators produce estimates closer to the mean in each sample.


---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 2: Low Sampling Variance (a.k.a. Efficiency).**

&lt;img src="03-Statistics_Review_files/figure-html/variance pdf-1.svg" style="display: block; margin: auto;" /&gt;

---
# Properties of Estimators

## __Sample__ Variance

- __Sample Variance:__ `\(S(Y_i)^2 = \frac{1}{n-1} \sum_{i=1}^{n}(Y_i - \bar{Y})^2\)`
  - __In Stata:__ Summarizing data.   `sum Y, detail`
- __Population Variance:__ `\(V(Y_i) = E\left[(Y_i - E[\bar{Y}])^2\right]=\sigma_Y^2\)`
  - Unknown parameter
- __Standard Deviation:__ Square root of the variance `\(\sigma_Y = \sqrt{\sigma_Y^2}\)`

--

## __Sampling__ Variance of `\(\hat{\mu}=\bar{Y}\)`
We want to characterize the variance of  `\(\bar{Y}\)` across repeated samples
+ `\(V(\bar{Y}) = E\left[(\bar{Y} - E[\bar{Y}])^2\right]=E\left[(\bar{Y} - E[Y_i])^2\right]\)`
+ By the unbiasedness property
+ `\(V(\bar{Y})\)`: variance of sample mean
+ `\(V(Y_i)\)` or `\(\sigma_Y^2\)`: population variance of underlying data
	

---
# Properties of Estimators
## __Sampling__ Variance of `\(\hat{\mu}=\bar{Y}\)`

- *Sampling* variance is related to *population* variance
`$$V(\bar{Y}) = V\left(\frac{1}{n}\sum_{i=1}^n{Y_i}\right) = \frac{1}{n^2} \sum_{i=1}^n V(Y_i)= \frac{1}{n^2} \sum_{i=1}^n \sigma_Y^2=\frac{n \sigma^2_Y}{n^2} = \frac{\sigma^2_Y}{n}$$`
	
  + Variance of a sum is the sum of variances
  + Constants are squared when pulled out of a variance
	
- Thus, sampling variance of an average depends on variance of underlying data and number of observations

---
# Properties of Estimators
## Standard Errors

- We usually work with standard deviation of sample mean rather than variances

- __Standard error__ is the standard deviation of an _estimator_&lt;sup&gt;+&lt;/sup&gt;

--

- `\(SE(\bar{Y}) = \sqrt{V(\bar{Y})} = \frac{\sigma_Y}{\sqrt{n}}\)`

- `\(\widehat{SE}(\bar{Y}) = \frac{S(Y_i)}{\sqrt{n}}\)`, Estimated Standard Error

- SE summarize variation in estimate from _random sampling_

- Again, SE `\(\neq\)` standard deviation of underlying data

.footnote[
&lt;sup&gt;+&lt;/sup&gt; The estimator we've considered so far is the sample average. More specifically, the standard error is the standard deviation of the _sampling distribution_ of an estimator.
]

---
# Properties of Estimators
## The Bias-Variance Tradeoff

Should we be willing to take a bit of bias to reduce the variance?

In econometrics, we generally prefer unbiased estimators. Some other disciplines think more about this tradeoff.

&lt;img src="03-Statistics_Review_files/figure-html/variance bias-1.svg" style="display: block; margin: auto;" /&gt;
---
# Properties of Estimators

**Question:** What properties make an estimator reliable?

**Answer 3: Consistency.**

- We want uncertainty of our estimator to decrease as `\(n\)` grows.
  + I.e., want probability that estimate `\(\hat{\mu}_Y\)` falls within a small interval around parameter `\(\mu\)` to get increasingly closer to 1 as `\(n\)` grows. 
  
- __Intuition:__ As `\(n\)` grows, our sample size approaches population size `\(\Rightarrow\)` uncertainty should fall

- This is the __Law of Large Numbers (LLN)__


---
# Law of Large Numbers (LLN)
## Law of Large Numbers
&gt; LLN implies that `\(\bar{Y}\)` will be very close to `\(E[Y_i]\)` as the sample size grows

- Let's empirically test LLN let's flip a fair coin 100,000 times 
- Record cumulative average (H=1) (T=0)
- `\(E[Y_i]=0.5\)`
---

# LLN


&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# LLN

&lt;img src="03-Statistics_Review_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# LLN - Analytic Proof

- We've shown that *sampling* variance can be written as
`$$V(\bar{Y}) = \frac{\sigma^2_Y}{n}$$`
	
- LLN at work, large `\(n\)` implies little dispersion
  
  + As `\(n\rightarrow \infty\)`, `\(V(\bar{Y})\rightarrow 0\)`

---
# LLN - One More Visualization
&lt;img src="figs/sampleDistNormal.gif" width="58%" style="display: block; margin: auto;" /&gt;
- __.red[Red line:]__ Mean of Sample. __.blue[Blue line:]__ Mean of Sampling Distribution. 
- __True Population Mean:__ `\(\mu=0\)`

---
# Unbiased Estimators

In addition to the sample mean and sample variance, there are several other unbiased estimators we will use often.

- __Sample covariance__ to estimate covariance `\(\sigma_{XY}\)`.

- __Sample correlation__ to estimate the population correlation coefficient `\(\rho_{XY}\)`.

---
# Unbiased Estimators

The sample covariance `\(S_{XY}\)` is an unbiased estimator of the population covariance `\(\sigma_{XY}\)`:


`$$S_{XY} = \dfrac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y}).$$`
---
# Unbiased Estimators

The sample correlation `\(r_{XY}\)` is an unbiased estimator of the population correlation coefficient `\(\rho_{XY}\)`:

`$$r_{XY} = \dfrac{S_{XY}}{\sqrt{S_X^2} \sqrt{S_Y^2}}.$$`
---
# Unbiased Estimators

## Poll Questions (1)

--

&lt;iframe src="https://embed.polleverywhere.com/multiple_choice_polls/mI9Mxi2TAHLao5ciGaEYF?controls=none&amp;short_poll=true" width="800" height="600" frameBorder="0"&gt;&lt;/iframe&gt;

---
#Unbiased Estimators

Sorry, lots of questions. It is so easy to lose the forest for the trees with all of these statistical concepts

## Group Questions
1. .pink[__How does the LLN help us learn about populations using samples?__]

1. __.pink[What is a standard error and why is it useful?]__

--

Okay, we are ready to actually be researchers and test some hypotheses! Next time.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

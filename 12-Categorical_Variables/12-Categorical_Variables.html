<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Categorical Variables</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Cook" />
    <link href="12-Categorical_Variables_files/remark-css/default.css" rel="stylesheet" />
    <link href="12-Categorical_Variables_files/remark-css/metropolis.css" rel="stylesheet" />
    <link href="12-Categorical_Variables_files/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Categorical Variables
## ECON 4651: Principles of Econometrics for Business and Analytics
### Jason Cook
### Fall 2020

---

class: white-slide

Instead of waiting in awkward silence for everyone to join, enjoy this live panda footage

&lt;iframe width="914" height="514" src="https://www.youtube.com/embed/Gm3bQVANtVo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;!-- install.packages(c("pagedown", "xaringan")) --&gt;
&lt;!-- pagedown::chrome_print("C:/Users/Jason/Box/Teaching/Jason Cook/QAMO_UG_Intro_Metrics/Lectures/12-Categorical_Variables/12-Categorical_Variables.html") --&gt;


---
class: inverse, middle



# Prologue

---
# Housekeeping

## Midterm

Review lecture on Tuesday (10/27).

Exam on Thursday (10/29).

- Because of new test format, no cheat sheet. .hi[Sorry!] But I will provide formula sheet (.hi[Posted on Canvas now])

- Sign-up for a midterm slot with [survey](https://forms.gle/oeUziaCoMKgDFKwC8)

**These slides are last of material covered on exam**

---
class: inverse, middle

# Categorical ("Dummy") Variables
--

&lt;img src="https://media.giphy.com/media/5xrkJe3IJKSze/source.gif" width="70%" style="display: block; margin: auto;" /&gt;

---
layout: true
# Categorical ("Dummy") Variables
---

**Goal:** Make quantitative statements about .pink[qualitative information].

- *e.g.,* race, gender, being employed, living in Utah, *etc.*

--

**Approach:** Construct .pink[binary variables].

- _a.k.a._ .pink[dummy variables] or .pink[indicator variables].
- Value equals 1 if observation is in the category or 0 if otherwise.

--

**Regression implications**

1. Binary variables change the interpretation of the intercept.

2. Coefficients on binary variables have different interpretations than those on continuous variables.

---
layout: true
# Continuous Variables
---

Consider the relationship

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{School}_i + u_i $$

where

- `\(\text{TestScores}_i\)` is a continuous variable measuring an individual's standardized test score
- `\(\text{School}_i\)` is a continuous variable that measures years of education

--

**Interpretation**

- `\(\beta_0\)`: `\(y\)`-intercept, _i.e._, `\(\text{TestScores}\)` when `\(\text{School} = 0\)`
- `\(\beta_1\)`: expected increase in `\(\text{TestScores}\)` for a one-unit increase in `\(\text{School}\)`

---

Consider the relationship

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{School}_i + u_i $$

**Derive the slope's interpretation:**

`\(\mathop{\mathbb{E}}\left[ \text{TestScores} | \text{School} = \ell + 1 \right] - \mathop{\mathbb{E}}\left[ \text{TestScores} | \text{School} = \ell \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1 (\ell + 1) + u \right] - \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1 \ell + u \right]\)`
--
&lt;br&gt; `\(\quad = \left[ \beta_0 + \beta_1 (\ell + 1) \right] - \left[ \beta_0 + \beta_1 \ell \right]\)`
--
&lt;br&gt; `\(\quad = \beta_0 - \beta_0 + \beta_1 \ell - \beta_1 \ell + \beta_1\)`
--
&lt;br&gt; `\(\quad = \beta_1\)`.

--

The slope gives the expected increase in test scores for an additional year of schooling.

---

Consider the relationship

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{School}_i + u_i $$

**Alternative derivation**

Differentiate the model with respect to schooling:

$$ \dfrac{d\text{TestScores}}{d\text{School}} = \beta_1 $$

The slope gives the expected increase in TestScores for an additional year of schooling.

---

If we have multiple explanatory variables, _e.g._,

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Ability}_i + u_i $$

then the interpretation changes slightly.

--

`\(\mathop{\mathbb{E}}\left[ \text{TestScore} | \text{School} = \ell + 1, \text{Ability} = \alpha \right] - \mathop{\mathbb{E}}\left[ \text{TestScore} | \text{School} = \ell, \text{Ability} = \alpha \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1 (\ell + 1) + \beta_2 \alpha + u \right] - \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1 \ell + \beta_2 \alpha + u \right]\)`
--
&lt;br&gt; `\(\quad = \left[ \beta_0 + \beta_1 (\ell + 1) + \beta_2 \alpha \right] - \left[ \beta_0 + \beta_1 \ell + \beta_2 \alpha \right]\)`
--
&lt;br&gt; `\(\quad = \beta_0 - \beta_0 + \beta_1 \ell - \beta_1 \ell + \beta_1 + \beta_2 \alpha - \beta_2 \alpha\)`
--
&lt;br&gt; `\(\quad = \beta_1\)`

--

The slope gives the expected increase in Test Scores for an additional year of schooling, **holding ability constant**.

---

If we have multiple explanatory variables, _e.g._,

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Ability}_i + u_i $$

then the interpretation changes slightly.

**Alternative derivation**

Differentiate the model with respect to schooling:

$$ \dfrac{\partial\text{TestScores}}{\partial\text{School}} = \beta_1 $$

The slope gives the expected increase in Test Scores for an additional year of schooling, **holding ability constant**.

---
layout: true
# Categorical ("Dummy") Variables
---


Consider the relationship

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{Female}_i + u_i $$

where `\(\text{TestScores}_i\)` is a continuous variable measuring an individual's 3rd grade math scores and `\(\text{Female}_i\)` is a binary variable equal to `\(1\)` if `\(i\)` identifies as female.

**Interpretation**

`\(\beta_0\)` is the expected `\(\text{TestScores}\)` for males and other gender identities (_i.e._, when `\(\text{Female} = 0\)`)&lt;sup&gt;**+**&lt;/sup&gt;:

.footnote[**+**: I'll abbreviate male and other gender identities with "Male+"]

`\(\mathop{\mathbb{E}}\left[ \text{TestScores} | \text{Male}^+ \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1\times 0 + u_i \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + 0 + u_i \right] \quad = \quad \beta_0\)`

---

Consider the relationship

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{Female}_i + u_i $$

where `\(\text{TestScores}_i\)` is a continuous variable measuring an individual's 3rd grade math scores and `\(\text{Female}_i\)` is a binary variable equal to `\(1\)` when `\(i\)` identifies as female.

**Interpretation**

`\(\beta_1\)` is the expected difference in `\(\text{TestScores}\)` between females and males+:

`\(\mathop{\mathbb{E}}\left[ \text{TestScores} | \text{Female} \right] - \mathop{\mathbb{E}}\left[ \text{TestScores} | \text{Male}^+ \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1\times 1 + u_i \right] - \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1\times 0 + u_i \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1 + u_i \right] - \mathop{\mathbb{E}}\left[ \beta_0 + 0 + u_i \right]\)`
--
&lt;br&gt; `\(\quad = \beta_0 + \beta_1 - \beta_0\)`
--
&lt;br&gt; `\(\quad = \beta_1\)`

---

Consider the relationship

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{Female}_i + u_i $$

where `\(\text{TestScores}_i\)` is a continuous variable measuring an individual's 3rd grade math scores and `\(\text{Female}_i\)` is a binary variable equal to `\(1\)` when `\(i\)` identifies as female.

**Interpretation**

`\(\beta_0 + \beta_1\)`: is the expected `\(\text{TestScores}\)` for females:

`\(\mathop{\mathbb{E}}\left[ \text{TestScores} | \text{Female} \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1\times 1 + u_i \right]\)`
--
&lt;br&gt; `\(\quad = \mathop{\mathbb{E}}\left[ \beta_0 + \beta_1 + u_i \right]\)`
--
&lt;br&gt; `\(\quad = \beta_0 + \beta_1\)`

---

Consider the relationship

$$ \text{TestScores}_i = \beta_0 + \beta_1 \text{Female}_i + u_i $$

**Interpretation**

- `\(\beta_0\)`: expected `\(\text{TestScores}\)` for males+ (_i.e._, when `\(\text{Female} = 0\)`)
- `\(\beta_1\)`: expected difference in `\(\text{TestScores}\)` between females and males+
- `\(\beta_0 + \beta_1\)`: expected `\(\text{TestScores}\)` for females
- Males+ are the **reference group**

--

**Note:** If there are no other variables to condition on, then `\(\hat{\beta}_1\)` equals the difference in group means, _e.g._, `\(\bar{X}_\text{Female} - \bar{X}_{\text{Male}^+}\)`.

--

**Note&lt;sub&gt;2&lt;/sub&gt;:** The *holding all other variables constant* interpretation also applies for categorical variables in multiple regression settings.

---

&lt;img src="figs/L9_bivar.png" width="75%" style="display: block; margin: auto;" /&gt;

---

.hi[Practice Problems (3)]
&lt;img src="figs/L9_Pract1.png" width="90%" style="display: block; margin: auto;" /&gt;
- `\(FRL\)` is a dummy equal to one if the student qualifies for free/reduced price lunch
  1. What is the average score for non-FRL-eligible students?
	1. What is the average score for FRL-eligible students?
	1. What is average score for FRL-eligible relative to non-FRL-eligible students?

---

&lt;iframe src="https://embed.polleverywhere.com/multiple_choice_polls/jfRxhNjXPCR3j3ZB8zcZT?controls=none&amp;short_poll=true" width="800" height="600" frameBorder="0"&gt;&lt;/iframe&gt;
---

&lt;iframe src="https://embed.polleverywhere.com/multiple_choice_polls/fPKRCWyKZm6phKOxNvBhT?controls=none&amp;short_poll=true" width="800" height="600" frameBorder="0"&gt;&lt;/iframe&gt;

---

&lt;iframe src="https://embed.polleverywhere.com/multiple_choice_polls/KHtfY0vgR7s1Zj4Bdn5jF?controls=none&amp;short_poll=true" width="800" height="600" frameBorder="0"&gt;&lt;/iframe&gt;
---

&lt;!-- - More generally we have --&gt;
&lt;!-- `$$Y_i = \beta_0 + \beta_1 Z_i + u_i$$` --&gt;
&lt;!-- - Conditional expectation of `\(Y_i\)` given a dummy `\(Z_i\)` is --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- &amp;E[Y_i|Z_i=0]=\beta_0\\ --&gt;
&lt;!-- &amp;E[Y_i|Z_i=1]=\beta_0 + \beta_1 --&gt;
&lt;!-- \end{align*} --&gt;
&lt;!-- so that --&gt;
&lt;!-- `$$\beta_1 = E[Y_i|Z_i=1] - E[Y_i|Z_i=0]$$` --&gt;
&lt;!-- is the difference in expected `\(Y_i\)` with the dummy on and off --&gt;

&lt;!-- --- --&gt;

&lt;!-- We can also write --&gt;
&lt;!-- \begin{align*} --&gt;
&lt;!-- E[Y_i|Z_i] &amp;= E[Y_i|Z_i=0]+(E[Y_i|Z_i=1] - E[Y_i|Z_i=0])Z_i\\ --&gt;
&lt;!-- &amp;=\beta_0 + \beta_1 Z_i --&gt;
&lt;!-- \end{align*} --&gt;

&lt;!-- - Shows that `\(E[Y_i|Z_i]\)` is linear function of `\(Z_i\)` with slope `\(\beta_1\)` and intercept `\(\beta_0\)` --&gt;

&lt;!-- - Because CEF with single dummy is linear, regression fits CEF perfectly. Thus, regression slope must also be `\(\beta_1=E[Y_i|Z_i=1] -E[Y_i|Z_i=0]\)` --&gt;

&lt;!--   - In words `\(\beta_1\)` gives the difference in average outcomes between both groups --&gt;

&lt;!-- --- --&gt;
&lt;!-- layout: true --&gt;
&lt;!-- # Interpreting Regressions with Dummies --&gt;
&lt;!-- --- --&gt;

&lt;!-- `$$Y_i = \beta_0 + \beta_1 Z_i + u_i$$` --&gt;

&lt;!-- - Because the dummy bivariate regression perfectly fits the CEF, it helps with intuition behind interpreting coefficients --&gt;

&lt;!-- \begin{align*} --&gt;
&lt;!-- &amp;E[Y_i|Z_i=0]=\beta_0\\ --&gt;
&lt;!-- &amp;E[Y_i|Z_i=1]=\beta_0 + \beta_1 --&gt;
&lt;!-- \end{align*} --&gt;

&lt;!-- - Thus, the intercept `\(\beta_0\)` gives the average `\(Y\)` value for people with `\(Z_i=0\)` and  `\(\beta_0 + \beta_1\)` gives the average `\(Y\)` value for people with `\(Z_i=1\)` --&gt;

&lt;!-- 	- True in general, intercept is average for group defined by all dummies `\(=0\)` --&gt;

---

`\(Y_i = \beta_0 + \beta_1 Z_i + u_i\)` for binary variable `\(Z_i = \{\color{#314f4f}{0}, \, \color{#e64173}{1}\}\)`



&lt;img src="12-Categorical_Variables_files/figure-html/dat plot 1-1.svg" style="display: block; margin: auto;" /&gt;

---

`\(Y_i = \beta_0 + \beta_1 Z_i + u_i\)` for binary variable `\(Z_i = \{\color{#314f4f}{0}, \, \color{#e64173}{1}\}\)`

&lt;img src="12-Categorical_Variables_files/figure-html/dat plot 2-1.svg" style="display: block; margin: auto;" /&gt;

---
layout: true
# Multiple Regression
---

This intuition extends to categorical variables that take on many values
`$$TestScore_i = \beta_0 + \beta_1 Age8_i + \beta_2 Age9_i + u_i$$`
- `\(\beta_0\)`: average score for 7-year-olds, i.e., group defined by all dummies `\(=0\)`

- `\(\beta_0+\beta_1\)`: average test score for 8-year-olds

.left-column[

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
.right-column[
&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---

`$$TestScore_i = \beta_0 + \beta_1 Age8_i + \beta_2 Age9_i + u_i$$`

- .hi[Group Questions:] What is interpretation of `\(\beta_2\)`? How would you calculate average score for 9-year-olds using regression coefficients?
.left-column[

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
.right-column[
&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
---

- If we change the set of dummies, the left-out group becomes the new intercept

`$$TestScore_i = \beta_0 + \beta_1 Age7_i + \beta_2 Age9_i + u_i$$`

.left-column[

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]
.right-column[

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---
class: white-slide, middle

We can also estimate models with both continuous and categorical variables

---




&lt;img src="12-Categorical_Variables_files/figure-html/mult reg plot 1-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

The intercept and categorical variable `\(Z\)` control for the groups' means.

&lt;img src="12-Categorical_Variables_files/figure-html/mult reg plot 2-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

With groups' means removed:

&lt;img src="12-Categorical_Variables_files/figure-html/mult reg plot 3-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

`\(\hat{\beta}_1\)` estimates the relationship between `\(Y\)` and `\(X\)` after controlling for `\(Z\)`.

&lt;img src="12-Categorical_Variables_files/figure-html/mult reg plot 4-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Another way to think about it:

&lt;img src="12-Categorical_Variables_files/figure-html/mult reg plot 5-1.svg" style="display: block; margin: auto;" /&gt;

---
class: white-slide

**Question:** Why not estimate `\(\text{TestScores}_i = \beta_0 + \beta_1 \text{Female}_i + \beta_2 \text{Male}^+_i  + u_i\)`?

--

**Answer:** The intercept is a perfect linear combination of `\(\text{Male}^+_i\)` and `\(\text{Female}_i\)`

- Violates .pink[no perfect collinearity] assumption.
  - .blue[**More on this later**]. Recall, this makes us divide by 0

- OLS can't estimate all three parameters simultaneously.

- Known as .hi[dummy variable trap].

**Practical solution:** Select a reference category and drop its indicator. 

---
layout: false
# Dummy Variable _Trap?_

Don't worry, .mono[Stata] will bail you out if you include perfectly collinear indicators.

**Example**

&lt;img src="figs/DVT.png" width="70%" style="display: block; margin: auto;" /&gt;

--

Thanks, .mono[Stata].

---
layout: true
# Omitted Variable Bias
---

Just for fun, let's revisit the concept of omitted variables bias in this context

Recall that **omitted variable bias** (OVB) arises when we omit a variable that

1. Affects the outcome variable `\(Y\)`

2. Correlates with an explanatory variable `\(X_j\)`

Biases OLS estimator of `\(\beta_j\)`.

---

**Example**

Let's imagine a simple population model for the amount individual `\(i\)` gets paid

$$ \text{Income}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Male}^+_i + u_i $$

where `\(\text{School}_i\)` gives `\(i\)`'s years of schooling and `\(\text{Male}^+_i\)` denotes an indicator variable for whether individual `\(i\)` is male or a gender identity other than female.

**Interpretation**

- `\(\beta_1\)`: returns to an additional year of schooling (*ceteris paribus*)
- `\(\beta_2\)`: premium for being male&lt;sup&gt;+&lt;/sup&gt; (*ceteris paribus*)
--

&lt;br&gt;If `\(\beta_2 &gt; 0\)`, then there is discrimination against women.

---

**Example, continued**

From the population model

$$ \text{Income}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Male}^+_i + u_i $$

An analyst focuses on the relationship between scores and schooling, _i.e._,

$$ \text{Income}_i = \beta_0 + \beta_1 \text{School}_i + \left(\beta_2 \text{Male}^+_i + u_i\right) $$
$$ \text{Income}_i = \beta_0 + \beta_1 \text{School}_i + \varepsilon_i $$

where `\(\varepsilon_i = \beta_2 \text{Male}^+_i + u_i\)`.

--

We assumed exogeneity to show that OLS is unbiasedness. But even if `\(\mathop{\mathbb{E}}\left[ u | X \right] = 0\)`, it is not necessarily true that `\(\mathop{\mathbb{E}}\left[ \varepsilon | X \right] = 0\)` (false if `\(\beta_2 \neq 0\)`).

--

Specifically, `\(\mathop{\mathbb{E}}\left[ \varepsilon | \text{Male}^+ = 1 \right] = \beta_2 + \mathop{\mathbb{E}}\left[ u | \text{Male}^+ = 1 \right] \neq 0\)`.
--
 **Now OLS is biased.**

---

Let's try to see this result graphically.



The true population model:

$$ \text{Income}_i = 20 + 0.5 \times \text{School}_i + 10 \times \text{Male}^+_i + u_i $$

The regression model that suffers from omitted-variable bias:

$$ \text{Income}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \text{School}_i + u_i $$

Finally, imagine that women, on average, receive more schooling than men.

---

True model: `\(\text{Income}_i = 20 + 0.5 \times \text{School}_i + 10 \times \text{Male}^+_i + u_i\)`

&lt;img src="12-Categorical_Variables_files/figure-html/plot ovb 1-1.svg" style="display: block; margin: auto;" /&gt;
---
count: false

Biased regression: `\(\widehat{\text{Income}}_i = 31.3 + -0.9 \times \text{School}_i\)`

&lt;img src="12-Categorical_Variables_files/figure-html/plot ovb 2-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Recalling the omitted variable: Gender (**&lt;font color="#e64173"&gt;female&lt;/font&gt;** and **&lt;font color="#314f4f"&gt;male&lt;/font&gt;**)

&lt;img src="12-Categorical_Variables_files/figure-html/plot ovb 3-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Recalling the omitted variable: Gender (**&lt;font color="#e64173"&gt;female&lt;/font&gt;** and **&lt;font color="#314f4f"&gt;male&lt;/font&gt;**)

&lt;img src="12-Categorical_Variables_files/figure-html/plot ovb 4-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Unbiased regression: `\(\widehat{\text{Income}}_i = 20.9 + 0.4 \times \text{School}_i + 9.1 \times \text{Male}^+_i\)`

&lt;img src="12-Categorical_Variables_files/figure-html/plot ovb 5-1.svg" style="display: block; margin: auto;" /&gt;

---
layout: true
class: inverse, middle
# Regression with Dummies as a Matching Estimator

---

- Regressions with Dummy controls helps to see why regressions act as matching estimators

- To see this, let's jump back to that complicated model estimating private school returns

---

&lt;img src="https://media.giphy.com/media/9oIZRd54OPX3dPNV6p/giphy.gif" width="70%" style="display: block; margin: auto;" /&gt;

---
layout: true
# Regression Anatomy with Dummies
---

- Consider a simplified model from our earlier private schooling example with only dummies

`$$Y_i = \alpha + \beta P_i + \sum_{j=1}^{150} \gamma_j GROUP_{ji} + u_i$$`
Recall that:
- `\(Y_i\)`: Earnings
- `\(P_i\)`: Private School Dummy
- `\(GROUP_{jt}\)`: Application set dummy

We are interested in estimating the causal relationship of .pink[private college attendance] on .pink[earnings]


---

`$$Y_i = \alpha + \beta P_i + \sum_{j=1}^{150} \gamma_j GROUP_{ji} + u_i$$`

Coefficient|Average Earnings Among:
----|----
`\(\alpha\)` | .green[**Public**] school students ( `\(P_i=0\)` ) in .pink[**group 0**]
--

`\(\beta\)` | .blue[**Private**] relative to .green[**public**] students in .pink[**group 0**]
--

`\(\gamma_j\)` | .green[**Public**] students in .orange[**group j**] relative to .pink[**group 0**]
--

`\(\alpha + \gamma_j\)` | .green[**Public**] school students ( `\(P_i=0\)` ) in .orange[**group j**]
--

`\(\alpha + \beta +\gamma_j\)` | .blue[**Private**] school students ( `\(P_i=1\)` ) in .orange[**group j**]
--

- Note that Private relative to public student earnings in .orange[**group j**] are also `\(\beta\)` (take difference of last two lines)
	- Linear CEF `\(\Rightarrow\beta\)` is private-relative-to-public earnings for each group
	- OLS acts as .pink[matching estimator] by comparing private to public earnings among students in same `\(GROUP\)`

---

`$$Y_i = \alpha + \beta P_i + \sum_{j=1}^{150} \gamma_j GROUP_{ji} + u_i$$`

- Regression anatomy tells us that `\(\beta\)` is the bivariate coefficient from a regression on `\(\tilde{P}_i\)`, i.e., `\(\beta=\frac{C(\tilde{Y}_i,\tilde{P}_i)}{V(\tilde{P}_i)}\)` , where
`$$P_i = \delta_0 + \sum_{j=1}^{150} \delta_j GROUP_{ji} + \tilde{P}_i$$`
--

- Add second subscript to index groups `\(j\)`, e.g., `\(Y_{ij}\)` is earnings of student `\(i\)` in selectivity group `\(j\)`

- Because auxiliary regression that generates `\(\tilde{P}_{ij}\)` has a parameter for every possible value of the CEF ( `\(E[P_i|GROUP_{1i},\cdots,GROUP_{150}]\)` ), the auxiliary regression perfectly fits the CEF


---

`$$P_i = \delta_0 + \sum_{j=1}^{150} \delta_j GROUP_{ji} + \tilde{P}_i$$`

- `\(\delta_0 + \delta_j\)` is the average private school attendance rate among students in group `\(j\)`
--

- Thus, the fitted value `\(\hat{P}_{ij}\)` from a regression of `\(P_{ij}\)` on the full set of group dummies is the mean private school attendance rate in each group, `\(\bar{P}_j\)`
`$$\hat{P}_{ij} = \delta_0 + \sum_{j=1}^{150} \delta_j GROUP_{ji}=\bar{P}_j$$`

- Because in general `\(u_i=Y_i-\hat{Y}_i\)`, the auxiliary regression residual is then `\(\tilde{P}_{ij}=P_{ij}-\bar{P}_j\)`



---

- Finally, regression anatomy gives
`$$\beta = \frac{C(\tilde{Y}_{ij},\tilde{P}_{ij})}{V(\tilde{P}_{ij})}=\frac{C(\tilde{Y}_{ij},P_{ij}-\bar{P}_j)}{V(\tilde{P}_{ij})}$$`

- Just as if we were to manually sort students into groups and compare public and private students, regression on private attendance with group controls is also a .hi[within-group] procedure

	- .pink[Variation across groups is removed] by subtracting `\(\bar{P}_j\)` to construct `\(\tilde{P}_{ij}\)`
	
	- Similar to how we dropped groups C and D, any group where everyone either attends private or public are .pink[uninformative] because `\(P_{ij} - \bar{P}_j=0\)`

---
class: inverse, middle
layout: false
# Let's look at this graphically
---

# Between and Within

- Let's pick a few groups and graph this out
- `\(Y_i\)` against `\(P_i\)` for a few groups

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-12-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# Between and Within

- If we analyze overall variation we get this
`$$Y_i = \alpha + \beta P_i + u_i$$`

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-13-1.png" width="80%" style="display: block; margin: auto;" /&gt;


---


# Between and Within

- .hi[Between] variation is what we get if we look at the relationship between the *means of each group*, i.e., `\(\bar{P}_j\)`

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-14-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

# Between and Within

- And I mean it! Only look at those means! The individual variation within groups doesn't matter.

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-15-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
class: clear-slide
layout: false

- .hi[Within] variation goes the other way - it treats those orange crosses as their own individualized sets of axes and only looks at variation *within* group across students!
- We basically slide the crosses over on top of each other and then analyze *that* data

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-16-1.gif" width="80%" style="display: block; margin: auto;" /&gt;

---
class: clear-slide
layout: false

## Both regressions are identical

`\(Y_i = \alpha + \beta P_i + \sum_{j=1}^{150} \gamma_j GROUP_{ji} + u_i\)`
.pink[and]
`\(\tilde{Y}_{i} = \alpha + \beta \tilde{P}_{i} + u_i\)`&lt;sup&gt;**+**&lt;/sup&gt; 

.footnote[**+:** where `\\(\tilde{Y}_i = Y_i - \bar{Y}_j\\)` and `\\(\tilde{P}_i = P_i - \bar{P}_j\\)`]

&lt;img src="12-Categorical_Variables_files/figure-html/unnamed-chunk-17-1.gif" width="80%" style="display: block; margin: auto;" /&gt;

---
class: clear-slide
layout: false

Okay, that was alot, how is everyone feeling?

&lt;img src="https://media.giphy.com/media/26DNdZwSGNdOXJ2py/source.gif" width="70%" style="display: block; margin: auto;" /&gt;

---

# Group Questions
Suppose that sort students into groups, `\(A\)` and `\(B\)`, based on the schools they apply and are accepted to. We regress `\(Y_i = \alpha + \beta P_i + \gamma A_i + u_i\)`

where everything is defined as before, but `\(A_i\)` is a binary equal to one for students in group `\(A\)`
1. Which (combo of) coefficient(s) gives average earnings among private students in group `\(A\)`?
1. Which (combo of) coefficient(s) gives average earnings among public students in group `\(A\)`?
1. Which (combo of) coefficient(s) gives average earnings among public students in group `\(B\)`?
1. Which (combo of) coefficient(s) gives average earnings among private relative to public students	in a given group?
1. Explain intuitively why this OLS regression acts as a matching estimator.

---
layout: false
class: clear-slide, middle

.huge[**Questions?**]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
